# Metal Flex Attention

<p align="center">
  <strong>Flex Attention Mechanism for MLX using Metal Kernels</strong>
</p>

<p align="center">
  <!-- Badges -->
  <a href="LICENSE"><img src="https://img.shields.io/badge/license-Apache%202.0-blue.svg" alt="License"></a>
</p>

<p align="center">
  <a href="#overview">Overview</a> â€¢
  <a href="#installation">Installation</a> â€¢
  <a href="#features">Features</a> â€¢
  <a href="#benchmarks">Benchmarks</a>
</p>

## Overview

Metal Flex Attention demonstrates the implementation of efficient attention mechanisms using the MLX framework with custom Metal source kernels. This implementation focuses on reducing memory usage and computational overhead, making it particularly suitable for large-scale models and processing long sequences.

## Installation

To get started with Metal Flex Attention:

```bash
git clone https://github.com/TheProxyCompany/metal-flex-attention
cd metal-flex-attention
pip install .
```

## Features

- ğŸš€ **Custom Fused Kernels**: Optimized performance through Metal-native implementation
- âš¡ **Efficient Computation**: Reduced memory usage and computational overhead
- ğŸ¯ **Sparse Attention Support**: Implements flexible attention patterns
- ğŸ’» **Metal Integration**: Native Metal implementation for Apple Silicon
- ğŸ”§ **MLX Framework**: Built on the MLX framework for optimal performance
- ğŸ“¦ **Easy Integration**: Simple setup and usage with existing projects

## Implementation Details

### Custom Kernels

The implementation leverages fused custom kernels for:
- 3D Tiled Matrix Multiplication


## Benchmarks

*(Benchmarks coming soon)*

## License

This project is licensed under the Apache 2.0 License. See the [LICENSE](LICENSE) file for details.

---

<p align="center">
  Made with care â¤ï¸ by The Proxy Company
</p>

<p align="center">
  <a href="https://x.com/whatisproxy">Twitter</a> â€¢
  <a href="https://www.what-is-proxy.com">Website</a> â€¢
  <a href="mailto:contact@what-is-proxy.com">Contact</a>
</p>
</p>


